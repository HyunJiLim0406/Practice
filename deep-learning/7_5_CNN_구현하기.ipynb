{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7.5 CNN 구현하기.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMsEhKGKbttUYe3f8rZmdQw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyunJiLim0406/Practice/blob/master/deep-learning/7_5_CNN_%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_FmXlXwwdqW"
      },
      "source": [
        "## __7.5 CNN 구현하기__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_1Gip3rwkE6",
        "outputId": "3058a3ac-cacb-4070-f854-2a81708be850"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2LQuzrEw9B9",
        "outputId": "312d0bb2-f690-4554-c6c0-3fe8b01edff3"
      },
      "source": [
        "%cd /content/drive/MyDrive/deep-learning-from-scratch-master/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/deep-learning-from-scratch-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quTjMtV9xsv8"
      },
      "source": [
        "class SimpleConvNet:\n",
        "  def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "    filter_num = conv_param['filter_num']\n",
        "    filter_size = conv_param['filter_size']\n",
        "    filter_pad = conv_param['pad']\n",
        "    filter_stride = conv_param['stride']\n",
        "    input_size = input_dim[1]\n",
        "    conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "    pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "    self.params['b1'] = np.zeros(filter_num)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
        "    self.params['b2'] = np.zeros(hidden_size)\n",
        "    self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['Conv1'] = Convolution(self.params['W1'], self.parmas['b1'], conv_param['stride'], conv_param['pad'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "    self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.layers['Relu2'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "    self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    return self.last_layer.forward(y, t)\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    # 순전파\n",
        "    self.loss(x, t)\n",
        "\n",
        "    # 역전파\n",
        "    dout = 1\n",
        "    dout = self.last_layer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    # 결과 저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Conv1'].dW\n",
        "    grads['b1'] = self.layers['Conv1'].db\n",
        "    grads['W2'] = self.layers['Affine1'].dW\n",
        "    grads['b2'] = self.layers['Affine1'].db\n",
        "    grads['W3'] = self.layers['Affine2'].dW\n",
        "    grads['b3'] = self.layers['Affine2'].db"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8e_rGrM2xBmg",
        "outputId": "2c4e12b6-a899-4dd2-897e-b954267b2c3b"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from ch07.simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# 데이터 읽기\n",
        "#(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.3003247153867146\n",
            "=== epoch:1, train acc:0.093, test acc:0.097 ===\n",
            "train loss:2.298353654265707\n",
            "train loss:2.2960261528364168\n",
            "train loss:2.2880940084115973\n",
            "train loss:2.2902038361649364\n",
            "train loss:2.2762819444418203\n",
            "train loss:2.2579350839259007\n",
            "train loss:2.2466557403899516\n",
            "train loss:2.2263345768335947\n",
            "train loss:2.2319028391115534\n",
            "train loss:2.195905471115834\n",
            "train loss:2.168969123050151\n",
            "train loss:2.1441503290849027\n",
            "train loss:2.1012045092455707\n",
            "train loss:2.0657074913580846\n",
            "train loss:2.002273226758181\n",
            "train loss:1.9141258005451771\n",
            "train loss:1.8840057129031171\n",
            "train loss:1.8318840239634648\n",
            "train loss:1.6912505102538853\n",
            "train loss:1.6386554451673176\n",
            "train loss:1.4717526818126367\n",
            "train loss:1.4824233313347648\n",
            "train loss:1.4017858146847828\n",
            "train loss:1.2761146618775627\n",
            "train loss:1.1296402148291045\n",
            "train loss:1.0920281809164558\n",
            "train loss:1.120360079746349\n",
            "train loss:1.0748863600112126\n",
            "train loss:0.9216711795115442\n",
            "train loss:0.9133308807973435\n",
            "train loss:0.7446637700754455\n",
            "train loss:0.7715929864546376\n",
            "train loss:0.8889109623858278\n",
            "train loss:0.7552144275436861\n",
            "train loss:0.7289120383394244\n",
            "train loss:0.6960665920129057\n",
            "train loss:0.6480780652833357\n",
            "train loss:0.8066895677496072\n",
            "train loss:0.6055852783255298\n",
            "train loss:0.4852489426342664\n",
            "train loss:0.6039959101947819\n",
            "train loss:0.5073563677847505\n",
            "train loss:0.6226973457196933\n",
            "train loss:0.5740136364640684\n",
            "train loss:0.5620103259902236\n",
            "train loss:0.6685981694446229\n",
            "train loss:0.6099602962018282\n",
            "train loss:0.6533402593140676\n",
            "train loss:0.4963642185917771\n",
            "train loss:0.34457219680450146\n",
            "=== epoch:2, train acc:0.815, test acc:0.806 ===\n",
            "train loss:0.4941617792481495\n",
            "train loss:0.48017720558425103\n",
            "train loss:0.6274009939593189\n",
            "train loss:0.4902117858792085\n",
            "train loss:0.4237260993587136\n",
            "train loss:0.3153784961472003\n",
            "train loss:0.4109853069106\n",
            "train loss:0.42617092966422815\n",
            "train loss:0.47468700668820163\n",
            "train loss:0.6335623326557154\n",
            "train loss:0.4760563721061892\n",
            "train loss:0.5468629357072934\n",
            "train loss:0.4172724462149668\n",
            "train loss:0.5032363831600005\n",
            "train loss:0.5076447939078121\n",
            "train loss:0.4979871602540183\n",
            "train loss:0.2484695065102585\n",
            "train loss:0.43002272524820123\n",
            "train loss:0.56651903571895\n",
            "train loss:0.3825260012974083\n",
            "train loss:0.4140600012445095\n",
            "train loss:0.43265774420086855\n",
            "train loss:0.3502536659372848\n",
            "train loss:0.4140672671733028\n",
            "train loss:0.3800038084631193\n",
            "train loss:0.5562122549105896\n",
            "train loss:0.4473147562903318\n",
            "train loss:0.39186664190091386\n",
            "train loss:0.38271667883984656\n",
            "train loss:0.2912602813529212\n",
            "train loss:0.3142478006855356\n",
            "train loss:0.34053167296607617\n",
            "train loss:0.6431452246872464\n",
            "train loss:0.39868414746928316\n",
            "train loss:0.31180468899408936\n",
            "train loss:0.3440583848150066\n",
            "train loss:0.36451428388071766\n",
            "train loss:0.3415199730863\n",
            "train loss:0.4794663740442003\n",
            "train loss:0.38955518655713556\n",
            "train loss:0.3200394795164574\n",
            "train loss:0.40915419893277294\n",
            "train loss:0.2610705094130484\n",
            "train loss:0.28594196562069224\n",
            "train loss:0.40762841073483075\n",
            "train loss:0.35596692198132573\n",
            "train loss:0.41057813433253154\n",
            "train loss:0.24729934400352185\n",
            "train loss:0.1529642993661986\n",
            "train loss:0.3265789300317408\n",
            "=== epoch:3, train acc:0.875, test acc:0.861 ===\n",
            "train loss:0.38213604281809405\n",
            "train loss:0.206010616418238\n",
            "train loss:0.3174124007011027\n",
            "train loss:0.3752845847741221\n",
            "train loss:0.2816864689188524\n",
            "train loss:0.3461767324908955\n",
            "train loss:0.2702349013082882\n",
            "train loss:0.47761409100690855\n",
            "train loss:0.5253346552687063\n",
            "train loss:0.42177902968350595\n",
            "train loss:0.303266731586662\n",
            "train loss:0.35776374716917003\n",
            "train loss:0.4167330804540177\n",
            "train loss:0.38967594071844325\n",
            "train loss:0.21692927780798554\n",
            "train loss:0.2943886247168115\n",
            "train loss:0.25211586685953163\n",
            "train loss:0.28914768275820546\n",
            "train loss:0.25754104682948953\n",
            "train loss:0.35888835685960324\n",
            "train loss:0.3120772634025054\n",
            "train loss:0.2727445639789626\n",
            "train loss:0.21922500082980853\n",
            "train loss:0.293580342352988\n",
            "train loss:0.3182400878983488\n",
            "train loss:0.18215277001313507\n",
            "train loss:0.2577219750648221\n",
            "train loss:0.3086194456916024\n",
            "train loss:0.31479017511422147\n",
            "train loss:0.3161372811576278\n",
            "train loss:0.14915544451099116\n",
            "train loss:0.24336105517955947\n",
            "train loss:0.31022859104140965\n",
            "train loss:0.3057453239300072\n",
            "train loss:0.22685264818219572\n",
            "train loss:0.1879156804478463\n",
            "train loss:0.23175975642527016\n",
            "train loss:0.29669036701805174\n",
            "train loss:0.30576256485680814\n",
            "train loss:0.22961542603092677\n",
            "train loss:0.20040661082180758\n",
            "train loss:0.2160164823265767\n",
            "train loss:0.2001284343742495\n",
            "train loss:0.1681747838688569\n",
            "train loss:0.14674260343314705\n",
            "train loss:0.2726296729623112\n",
            "train loss:0.2487056465824044\n",
            "train loss:0.12068724338827846\n",
            "train loss:0.2171644703018616\n",
            "train loss:0.2633239904290016\n",
            "=== epoch:4, train acc:0.906, test acc:0.882 ===\n",
            "train loss:0.3050412172946945\n",
            "train loss:0.2835443168311488\n",
            "train loss:0.20140882845446725\n",
            "train loss:0.3361551019528356\n",
            "train loss:0.21529562571234678\n",
            "train loss:0.2638318025819736\n",
            "train loss:0.1771420509278427\n",
            "train loss:0.20195720022334288\n",
            "train loss:0.20662288728243408\n",
            "train loss:0.30079024126051834\n",
            "train loss:0.2823888021532284\n",
            "train loss:0.23631771275754163\n",
            "train loss:0.11900064424688715\n",
            "train loss:0.3407778074227851\n",
            "train loss:0.24369244339278165\n",
            "train loss:0.3790060925751091\n",
            "train loss:0.35029459900827137\n",
            "train loss:0.23042895126060065\n",
            "train loss:0.3053028536801945\n",
            "train loss:0.20487134117060116\n",
            "train loss:0.19966177304714275\n",
            "train loss:0.2528809129110555\n",
            "train loss:0.19651071779847507\n",
            "train loss:0.2656902558536409\n",
            "train loss:0.31833177337843127\n",
            "train loss:0.2441179016799847\n",
            "train loss:0.2100037581503808\n",
            "train loss:0.25953790385081366\n",
            "train loss:0.3757169709143737\n",
            "train loss:0.17402591060791867\n",
            "train loss:0.2776872575987038\n",
            "train loss:0.28844897447645407\n",
            "train loss:0.2104521153416635\n",
            "train loss:0.17853312815176345\n",
            "train loss:0.3350185538588571\n",
            "train loss:0.2871674695419016\n",
            "train loss:0.23807553708901288\n",
            "train loss:0.31986083764121753\n",
            "train loss:0.16144783479880956\n",
            "train loss:0.30698647964553955\n",
            "train loss:0.31218842873222646\n",
            "train loss:0.3947956769848144\n",
            "train loss:0.15466936899007908\n",
            "train loss:0.30150617547877234\n",
            "train loss:0.18402558895800056\n",
            "train loss:0.2437445347820343\n",
            "train loss:0.26435411584701407\n",
            "train loss:0.15047145523123245\n",
            "train loss:0.21181539196812474\n",
            "train loss:0.2643407759747456\n",
            "=== epoch:5, train acc:0.92, test acc:0.899 ===\n",
            "train loss:0.3892996045945472\n",
            "train loss:0.2455744349832122\n",
            "train loss:0.20432695853965413\n",
            "train loss:0.2521749478509769\n",
            "train loss:0.16250417507931186\n",
            "train loss:0.2797486628578025\n",
            "train loss:0.10794614522136117\n",
            "train loss:0.2740150349665463\n",
            "train loss:0.25880879258846884\n",
            "train loss:0.15653484953984065\n",
            "train loss:0.21009831464659612\n",
            "train loss:0.23762079528767185\n",
            "train loss:0.29209715121778723\n",
            "train loss:0.12362431203058889\n",
            "train loss:0.25925029358482077\n",
            "train loss:0.2604666664492856\n",
            "train loss:0.36807719831531266\n",
            "train loss:0.17845393143886845\n",
            "train loss:0.11222601463033025\n",
            "train loss:0.20870354609452846\n",
            "train loss:0.2561789602090238\n",
            "train loss:0.2443366261063728\n",
            "train loss:0.4209008311574641\n",
            "train loss:0.19194847278271118\n",
            "train loss:0.19745633142124536\n",
            "train loss:0.2052599725695106\n",
            "train loss:0.2544410857069978\n",
            "train loss:0.23798527101271336\n",
            "train loss:0.20500102473269557\n",
            "train loss:0.2664444423552265\n",
            "train loss:0.13490413600964843\n",
            "train loss:0.17015686024434154\n",
            "train loss:0.09679784218721206\n",
            "train loss:0.11424439345924256\n",
            "train loss:0.20544910572105365\n",
            "train loss:0.18263559361623827\n",
            "train loss:0.1570826844192861\n",
            "train loss:0.18955092039050625\n",
            "train loss:0.1601198566678007\n",
            "train loss:0.1852110317804926\n",
            "train loss:0.18404226901040044\n",
            "train loss:0.19901618516472044\n",
            "train loss:0.24884779866207396\n",
            "train loss:0.15353431579570403\n",
            "train loss:0.12299980417417565\n",
            "train loss:0.2043439999132274\n",
            "train loss:0.17591878486830187\n",
            "train loss:0.2019667652038151\n",
            "train loss:0.2503824414003958\n",
            "train loss:0.1681257185236805\n",
            "=== epoch:6, train acc:0.93, test acc:0.905 ===\n",
            "train loss:0.3697615711887792\n",
            "train loss:0.16791829288401733\n",
            "train loss:0.1880846181732476\n",
            "train loss:0.29498563157147145\n",
            "train loss:0.1176006134892102\n",
            "train loss:0.13207755928537593\n",
            "train loss:0.20174658468775278\n",
            "train loss:0.18092408882691383\n",
            "train loss:0.2944988178996539\n",
            "train loss:0.3141222967306075\n",
            "train loss:0.21005286692583458\n",
            "train loss:0.1697619781175691\n",
            "train loss:0.20750981670223162\n",
            "train loss:0.22611576420123128\n",
            "train loss:0.2824695394912766\n",
            "train loss:0.2626471011426697\n",
            "train loss:0.21784226218354366\n",
            "train loss:0.20651446978104296\n",
            "train loss:0.18519845970502152\n",
            "train loss:0.11306203075525686\n",
            "train loss:0.27378816201560197\n",
            "train loss:0.15643269805185836\n",
            "train loss:0.18688154631015316\n",
            "train loss:0.14678917139140793\n",
            "train loss:0.1437408819721837\n",
            "train loss:0.07074295134642766\n",
            "train loss:0.26367186175007684\n",
            "train loss:0.2611556379915705\n",
            "train loss:0.1900521590592798\n",
            "train loss:0.08579061125382113\n",
            "train loss:0.20497521479550443\n",
            "train loss:0.15837288792269838\n",
            "train loss:0.16885938948237453\n",
            "train loss:0.1013880176472257\n",
            "train loss:0.11127369701065701\n",
            "train loss:0.1698227235251837\n",
            "train loss:0.12400126651019024\n",
            "train loss:0.26008638947907675\n",
            "train loss:0.15138723652752184\n",
            "train loss:0.21244844552457084\n",
            "train loss:0.2279125043747457\n",
            "train loss:0.1963808792982611\n",
            "train loss:0.07395376657396571\n",
            "train loss:0.1559865525115537\n",
            "train loss:0.10875834421519254\n",
            "train loss:0.11255236940307001\n",
            "train loss:0.16502756378507397\n",
            "train loss:0.17429042333668449\n",
            "train loss:0.22321262833355815\n",
            "train loss:0.08557780864119793\n",
            "=== epoch:7, train acc:0.935, test acc:0.916 ===\n",
            "train loss:0.1051520947641072\n",
            "train loss:0.12340193829101663\n",
            "train loss:0.15245846968585197\n",
            "train loss:0.1464665286974581\n",
            "train loss:0.17125698396008734\n",
            "train loss:0.14413480510454157\n",
            "train loss:0.07862752874615897\n",
            "train loss:0.2502155171306903\n",
            "train loss:0.2744984712378403\n",
            "train loss:0.30931597495753754\n",
            "train loss:0.16625228074056814\n",
            "train loss:0.12293556710586567\n",
            "train loss:0.15565235274572373\n",
            "train loss:0.12961734293196195\n",
            "train loss:0.1567945222249444\n",
            "train loss:0.11512872341399724\n",
            "train loss:0.11289711663699603\n",
            "train loss:0.10880144381206545\n",
            "train loss:0.13369297715483955\n",
            "train loss:0.11511920609413318\n",
            "train loss:0.16489785737394366\n",
            "train loss:0.17304968542934865\n",
            "train loss:0.23095561168580456\n",
            "train loss:0.18023656157450912\n",
            "train loss:0.15391591606286767\n",
            "train loss:0.10491978161714571\n",
            "train loss:0.08154116981393247\n",
            "train loss:0.1776240199191058\n",
            "train loss:0.07816238106293176\n",
            "train loss:0.15944422742782874\n",
            "train loss:0.1372097377353386\n",
            "train loss:0.20064647663002003\n",
            "train loss:0.14509820122574676\n",
            "train loss:0.13471695711229115\n",
            "train loss:0.1470538664994156\n",
            "train loss:0.13812557049806387\n",
            "train loss:0.1450105113144936\n",
            "train loss:0.21680126235902328\n",
            "train loss:0.20335547517339336\n",
            "train loss:0.05681644604470539\n",
            "train loss:0.06527056941318275\n",
            "train loss:0.1706556337247888\n",
            "train loss:0.12045748728989668\n",
            "train loss:0.18093885995103637\n",
            "train loss:0.10032233353060194\n",
            "train loss:0.1687953811561968\n",
            "train loss:0.18185120357204226\n",
            "train loss:0.10424572050726814\n",
            "train loss:0.14361743648469516\n",
            "train loss:0.13695536304286074\n",
            "=== epoch:8, train acc:0.94, test acc:0.922 ===\n",
            "train loss:0.22970089009463468\n",
            "train loss:0.18168384145575747\n",
            "train loss:0.08260801921597084\n",
            "train loss:0.10059231644460948\n",
            "train loss:0.15272405343469544\n",
            "train loss:0.10098825971463482\n",
            "train loss:0.0991959373013463\n",
            "train loss:0.15452059159643763\n",
            "train loss:0.16891667473766755\n",
            "train loss:0.08250896161730725\n",
            "train loss:0.1845923032431723\n",
            "train loss:0.21641370682461666\n",
            "train loss:0.09653235685362263\n",
            "train loss:0.21965811470091967\n",
            "train loss:0.18179140199238691\n",
            "train loss:0.08285424672778621\n",
            "train loss:0.09818295887813712\n",
            "train loss:0.09930888873853995\n",
            "train loss:0.25329606729146936\n",
            "train loss:0.16474299613906357\n",
            "train loss:0.2138553612776213\n",
            "train loss:0.15808739009640782\n",
            "train loss:0.16397394712416122\n",
            "train loss:0.05697415570772485\n",
            "train loss:0.10612019497146777\n",
            "train loss:0.1590418537407818\n",
            "train loss:0.19940087549119034\n",
            "train loss:0.1919617951930378\n",
            "train loss:0.0674946022517485\n",
            "train loss:0.2079508571460317\n",
            "train loss:0.2600234278058216\n",
            "train loss:0.08708673745538371\n",
            "train loss:0.08335581017219644\n",
            "train loss:0.13404715565834963\n",
            "train loss:0.08183724057835706\n",
            "train loss:0.11419113287613801\n",
            "train loss:0.12246318933493618\n",
            "train loss:0.13410426374190507\n",
            "train loss:0.20166183977638175\n",
            "train loss:0.06566480012011565\n",
            "train loss:0.070619575814159\n",
            "train loss:0.14771365847521767\n",
            "train loss:0.08755411393885977\n",
            "train loss:0.1359424417371728\n",
            "train loss:0.1605877350000999\n",
            "train loss:0.22988103151669428\n",
            "train loss:0.10305934482706171\n",
            "train loss:0.11820006734752475\n",
            "train loss:0.14238493459130605\n",
            "train loss:0.06623986278105026\n",
            "=== epoch:9, train acc:0.961, test acc:0.936 ===\n",
            "train loss:0.08518586334012762\n",
            "train loss:0.0969941959966933\n",
            "train loss:0.09032410227377867\n",
            "train loss:0.0691981586327855\n",
            "train loss:0.11026577037731476\n",
            "train loss:0.1307646016296456\n",
            "train loss:0.14445638785301135\n",
            "train loss:0.060692799463193815\n",
            "train loss:0.15849051372466907\n",
            "train loss:0.07542941786212351\n",
            "train loss:0.16150957711565642\n",
            "train loss:0.12306869939161849\n",
            "train loss:0.10569247065518338\n",
            "train loss:0.08749130753200078\n",
            "train loss:0.06399788138473139\n",
            "train loss:0.05625492181136984\n",
            "train loss:0.07480893710218028\n",
            "train loss:0.05714773824103173\n",
            "train loss:0.12967316874175724\n",
            "train loss:0.21099560425057448\n",
            "train loss:0.15939673146828073\n",
            "train loss:0.07410563391156176\n",
            "train loss:0.0995601044932168\n",
            "train loss:0.1411808103066411\n",
            "train loss:0.0828445698143871\n",
            "train loss:0.07693848535168435\n",
            "train loss:0.15265336913306185\n",
            "train loss:0.09107969462477909\n",
            "train loss:0.08813480926759226\n",
            "train loss:0.057788886527757134\n",
            "train loss:0.06371324952307102\n",
            "train loss:0.08274332593436778\n",
            "train loss:0.05059205746249226\n",
            "train loss:0.1010196158682755\n",
            "train loss:0.09702619313134622\n",
            "train loss:0.14392550733685078\n",
            "train loss:0.10424492413348242\n",
            "train loss:0.1952017747612649\n",
            "train loss:0.18363168611125688\n",
            "train loss:0.17031410129553265\n",
            "train loss:0.17829767367215724\n",
            "train loss:0.08584594145129715\n",
            "train loss:0.071213767319515\n",
            "train loss:0.1754534102310407\n",
            "train loss:0.13237201106759022\n",
            "train loss:0.11414671311256226\n",
            "train loss:0.1453761897318382\n",
            "train loss:0.11851174949170394\n",
            "train loss:0.09392469304644731\n",
            "train loss:0.0744055547400388\n",
            "=== epoch:10, train acc:0.959, test acc:0.94 ===\n",
            "train loss:0.12816203683665567\n",
            "train loss:0.053192433495554255\n",
            "train loss:0.059058756703853445\n",
            "train loss:0.10286171883358922\n",
            "train loss:0.15510923053797995\n",
            "train loss:0.17839298650024843\n",
            "train loss:0.09022805472578715\n",
            "train loss:0.04473888135733731\n",
            "train loss:0.21836234434746704\n",
            "train loss:0.1319459641466861\n",
            "train loss:0.18996772565547182\n",
            "train loss:0.10636328405344131\n",
            "train loss:0.0907628722627797\n",
            "train loss:0.13987460325681111\n",
            "train loss:0.125440356647122\n",
            "train loss:0.17087436946845494\n",
            "train loss:0.09354842540348589\n",
            "train loss:0.17676808746917572\n",
            "train loss:0.09741704683882967\n",
            "train loss:0.1818070614720619\n",
            "train loss:0.12039410334121456\n",
            "train loss:0.148168414533389\n",
            "train loss:0.1122570381022293\n",
            "train loss:0.09030463475635074\n",
            "train loss:0.14811276344317467\n",
            "train loss:0.0851473653527915\n",
            "train loss:0.10379816824514876\n",
            "train loss:0.14999513976156764\n",
            "train loss:0.1832027220632158\n",
            "train loss:0.052220709582215426\n",
            "train loss:0.07830806662499178\n",
            "train loss:0.06311610474200158\n",
            "train loss:0.1582704610071598\n",
            "train loss:0.05083536203301267\n",
            "train loss:0.08071429406432493\n",
            "train loss:0.07322983790774287\n",
            "train loss:0.11242825763491812\n",
            "train loss:0.15224719302969464\n",
            "train loss:0.0628972998900102\n",
            "train loss:0.10388299050120749\n",
            "train loss:0.09620421607378148\n",
            "train loss:0.07925622463519102\n",
            "train loss:0.10481835352642765\n",
            "train loss:0.06783382215173982\n",
            "train loss:0.05159099092249213\n",
            "train loss:0.0626899901554487\n",
            "train loss:0.12429302754069971\n",
            "train loss:0.0835844418934975\n",
            "train loss:0.13502821035371226\n",
            "train loss:0.11178334142259486\n",
            "=== epoch:11, train acc:0.964, test acc:0.943 ===\n",
            "train loss:0.07438356071488525\n",
            "train loss:0.14557582724502077\n",
            "train loss:0.10010978475309598\n",
            "train loss:0.10783534023121812\n",
            "train loss:0.04767927543355416\n",
            "train loss:0.11779312281991895\n",
            "train loss:0.09089680497494275\n",
            "train loss:0.07164299989737537\n",
            "train loss:0.03355922996405755\n",
            "train loss:0.07989781939184573\n",
            "train loss:0.18093614796082072\n",
            "train loss:0.11165804132339018\n",
            "train loss:0.04890622918222687\n",
            "train loss:0.05896521317659945\n",
            "train loss:0.1588408402301273\n",
            "train loss:0.1558368184303806\n",
            "train loss:0.11103326776090157\n",
            "train loss:0.043075900433679586\n",
            "train loss:0.09787841669614016\n",
            "train loss:0.06219823763737956\n",
            "train loss:0.07461553928761736\n",
            "train loss:0.12986839877168896\n",
            "train loss:0.060447978713669095\n",
            "train loss:0.08913377617996397\n",
            "train loss:0.13210741548356356\n",
            "train loss:0.042605787769294086\n",
            "train loss:0.03016284523649212\n",
            "train loss:0.13486699746990158\n",
            "train loss:0.07541571892816572\n",
            "train loss:0.07428493047989985\n",
            "train loss:0.14683473203014508\n",
            "train loss:0.0866161202689649\n",
            "train loss:0.06329266567957531\n",
            "train loss:0.10322119325174768\n",
            "train loss:0.06567727754344212\n",
            "train loss:0.15622951200369964\n",
            "train loss:0.09268434198639182\n",
            "train loss:0.08424791577164341\n",
            "train loss:0.062193771324230186\n",
            "train loss:0.14585649519830557\n",
            "train loss:0.08202700088634952\n",
            "train loss:0.07211043508022495\n",
            "train loss:0.1455228330535025\n",
            "train loss:0.08763225008983616\n",
            "train loss:0.1723511832744159\n",
            "train loss:0.04188691336913682\n",
            "train loss:0.10620943957354745\n",
            "train loss:0.06561079804843169\n",
            "train loss:0.07588974175830249\n",
            "train loss:0.1850326681415235\n",
            "=== epoch:12, train acc:0.964, test acc:0.937 ===\n",
            "train loss:0.06673139259693027\n",
            "train loss:0.04250645852295131\n",
            "train loss:0.06100493591554567\n",
            "train loss:0.043542935381754\n",
            "train loss:0.04637050912508932\n",
            "train loss:0.10008667550054504\n",
            "train loss:0.039092626156531064\n",
            "train loss:0.11274972173897371\n",
            "train loss:0.08797761389256388\n",
            "train loss:0.04819775147006622\n",
            "train loss:0.1250981849375543\n",
            "train loss:0.04972602028776788\n",
            "train loss:0.04694640177635834\n",
            "train loss:0.1632931217385761\n",
            "train loss:0.13499880303608466\n",
            "train loss:0.08524996513498566\n",
            "train loss:0.1036782815423326\n",
            "train loss:0.09385710261565741\n",
            "train loss:0.15704848589336845\n",
            "train loss:0.10955846942611604\n",
            "train loss:0.09734717197641082\n",
            "train loss:0.06720691983626541\n",
            "train loss:0.11645759109685659\n",
            "train loss:0.04162948104022694\n",
            "train loss:0.08379430030382086\n",
            "train loss:0.06317923310359619\n",
            "train loss:0.09183381884509631\n",
            "train loss:0.06237656325232921\n",
            "train loss:0.04698849596193123\n",
            "train loss:0.0591775353873744\n",
            "train loss:0.033114041877558766\n",
            "train loss:0.21377575203339855\n",
            "train loss:0.06972198353459837\n",
            "train loss:0.11047722768416263\n",
            "train loss:0.08339065992453015\n",
            "train loss:0.052898173738093884\n",
            "train loss:0.09906577916960888\n",
            "train loss:0.13042310171374047\n",
            "train loss:0.13656888766976935\n",
            "train loss:0.10448655063733511\n",
            "train loss:0.08306179797116416\n",
            "train loss:0.0747885734108168\n",
            "train loss:0.08589213400442014\n",
            "train loss:0.04075941232006887\n",
            "train loss:0.0714441358220025\n",
            "train loss:0.057802320285159484\n",
            "train loss:0.09223806397370772\n",
            "train loss:0.02811198773615002\n",
            "train loss:0.03673235222272533\n",
            "train loss:0.07398963690467796\n",
            "=== epoch:13, train acc:0.965, test acc:0.946 ===\n",
            "train loss:0.06893253582081414\n",
            "train loss:0.04275353944272596\n",
            "train loss:0.04726562601206633\n",
            "train loss:0.08501637784340801\n",
            "train loss:0.08634844886250834\n",
            "train loss:0.025689658909606696\n",
            "train loss:0.031543226673793834\n",
            "train loss:0.19704498625776862\n",
            "train loss:0.07984125638289377\n",
            "train loss:0.07287709598281912\n",
            "train loss:0.08238600294325604\n",
            "train loss:0.11478903273835246\n",
            "train loss:0.11884788300160382\n",
            "train loss:0.03348642896656143\n",
            "train loss:0.03292698321041753\n",
            "train loss:0.09523743796917715\n",
            "train loss:0.059302339515690895\n",
            "train loss:0.09718604235916278\n",
            "train loss:0.03852980697978569\n",
            "train loss:0.053971623464014254\n",
            "train loss:0.13499899058649067\n",
            "train loss:0.10327139065650934\n",
            "train loss:0.03445527601338812\n",
            "train loss:0.05108273370608618\n",
            "train loss:0.08130488362422925\n",
            "train loss:0.100262362765478\n",
            "train loss:0.04192865425314255\n",
            "train loss:0.12097957507487177\n",
            "train loss:0.09195821048594026\n",
            "train loss:0.0677451697222638\n",
            "train loss:0.08041685066114537\n",
            "train loss:0.04698943737187438\n",
            "train loss:0.04638362785177965\n",
            "train loss:0.039613771479717536\n",
            "train loss:0.05486617455145925\n",
            "train loss:0.06807869059947963\n",
            "train loss:0.024435358870884057\n",
            "train loss:0.05817441571800783\n",
            "train loss:0.05287918807450664\n",
            "train loss:0.059593419580954404\n",
            "train loss:0.09262273978535848\n",
            "train loss:0.09890727734918567\n",
            "train loss:0.027608971323257306\n",
            "train loss:0.08733379551760816\n",
            "train loss:0.05603660531434935\n",
            "train loss:0.12579046137274155\n",
            "train loss:0.06041405582150767\n",
            "train loss:0.03277976903198736\n",
            "train loss:0.057919432383866855\n",
            "train loss:0.04534561100109687\n",
            "=== epoch:14, train acc:0.976, test acc:0.957 ===\n",
            "train loss:0.08097521139685608\n",
            "train loss:0.0796547916683564\n",
            "train loss:0.06234287772675872\n",
            "train loss:0.08334623860240327\n",
            "train loss:0.04171299838048125\n",
            "train loss:0.08973531982452786\n",
            "train loss:0.08169738433631378\n",
            "train loss:0.028424720842163666\n",
            "train loss:0.06155403314035659\n",
            "train loss:0.04086172170644884\n",
            "train loss:0.030455200278121052\n",
            "train loss:0.059008574607857245\n",
            "train loss:0.11889365735997362\n",
            "train loss:0.053257679593358975\n",
            "train loss:0.03620414045096896\n",
            "train loss:0.04937056615105115\n",
            "train loss:0.07800204920315652\n",
            "train loss:0.06595254261639373\n",
            "train loss:0.051014764702946185\n",
            "train loss:0.048989314708141905\n",
            "train loss:0.10458708493755792\n",
            "train loss:0.03256642706661405\n",
            "train loss:0.04694741058312516\n",
            "train loss:0.11836879372551373\n",
            "train loss:0.09115607503298495\n",
            "train loss:0.03067588171874383\n",
            "train loss:0.09057388579277477\n",
            "train loss:0.05677511378499057\n",
            "train loss:0.04001986202790524\n",
            "train loss:0.03737956940458741\n",
            "train loss:0.04716481908447549\n",
            "train loss:0.05128893570889574\n",
            "train loss:0.04176047534703997\n",
            "train loss:0.06220035710650274\n",
            "train loss:0.03679533180417773\n",
            "train loss:0.026549357238525945\n",
            "train loss:0.06004607060084759\n",
            "train loss:0.07289536859969008\n",
            "train loss:0.09166038298362546\n",
            "train loss:0.09778890742114646\n",
            "train loss:0.06925407970100472\n",
            "train loss:0.051080319921061906\n",
            "train loss:0.059416614127344565\n",
            "train loss:0.044579779286546106\n",
            "train loss:0.055079253535916035\n",
            "train loss:0.0243805294397975\n",
            "train loss:0.0720238469398721\n",
            "train loss:0.0366148479227686\n",
            "train loss:0.03141688990429617\n",
            "train loss:0.08421910649936816\n",
            "=== epoch:15, train acc:0.981, test acc:0.958 ===\n",
            "train loss:0.07062205575267486\n",
            "train loss:0.06012961624482258\n",
            "train loss:0.04651406778029786\n",
            "train loss:0.061809389393998576\n",
            "train loss:0.03390755234083289\n",
            "train loss:0.01813538711304717\n",
            "train loss:0.06250768973879993\n",
            "train loss:0.02451703709775626\n",
            "train loss:0.055495670251162525\n",
            "train loss:0.03402837121249324\n",
            "train loss:0.03755759802293748\n",
            "train loss:0.032880712872777176\n",
            "train loss:0.04497044117343386\n",
            "train loss:0.07524256812736957\n",
            "train loss:0.08482327075825764\n",
            "train loss:0.027970533225564913\n",
            "train loss:0.04930664530917674\n",
            "train loss:0.035366622801124896\n",
            "train loss:0.03991063552448623\n",
            "train loss:0.04307311488804693\n",
            "train loss:0.10299480784442534\n",
            "train loss:0.06096946509952713\n",
            "train loss:0.031449432971021915\n",
            "train loss:0.03560534140585158\n",
            "train loss:0.05446860172565383\n",
            "train loss:0.05178625418887156\n",
            "train loss:0.04482322095675995\n",
            "train loss:0.07082367716957699\n",
            "train loss:0.026754230619818093\n",
            "train loss:0.10011105540110872\n",
            "train loss:0.03048873635808738\n",
            "train loss:0.06345700804760798\n",
            "train loss:0.04048736713986512\n",
            "train loss:0.024108965336059784\n",
            "train loss:0.04448088582835164\n",
            "train loss:0.06789938872794953\n",
            "train loss:0.04526132394215856\n",
            "train loss:0.0814286110780306\n",
            "train loss:0.03973366529130719\n",
            "train loss:0.03493786225897523\n",
            "train loss:0.04913107739060342\n",
            "train loss:0.07702131145799226\n",
            "train loss:0.030149517218865968\n",
            "train loss:0.021847189908692298\n",
            "train loss:0.058196666690263736\n",
            "train loss:0.024585976935041692\n",
            "train loss:0.021741612865669578\n",
            "train loss:0.1243410032789316\n",
            "train loss:0.024536922797824993\n",
            "train loss:0.06153946525449092\n",
            "=== epoch:16, train acc:0.981, test acc:0.956 ===\n",
            "train loss:0.028452978232894742\n",
            "train loss:0.017554623564330823\n",
            "train loss:0.03482784181343867\n",
            "train loss:0.03381174820975926\n",
            "train loss:0.02408791399245311\n",
            "train loss:0.07366424477978067\n",
            "train loss:0.022036411744921002\n",
            "train loss:0.02948650831986386\n",
            "train loss:0.04118004625694615\n",
            "train loss:0.06206909752814202\n",
            "train loss:0.0399168029461062\n",
            "train loss:0.029045903677279973\n",
            "train loss:0.02615350695674349\n",
            "train loss:0.19720087928319097\n",
            "train loss:0.048383846972157044\n",
            "train loss:0.027132886423692743\n",
            "train loss:0.06283997637335052\n",
            "train loss:0.025523667936432567\n",
            "train loss:0.0449240960152199\n",
            "train loss:0.028132552412401347\n",
            "train loss:0.01750971925752603\n",
            "train loss:0.025489321975934384\n",
            "train loss:0.024390506513044374\n",
            "train loss:0.0338580165083467\n",
            "train loss:0.02862672634800177\n",
            "train loss:0.1082626351955747\n",
            "train loss:0.010904965044762039\n",
            "train loss:0.049040308538921495\n",
            "train loss:0.04055956202435054\n",
            "train loss:0.060267206815900344\n",
            "train loss:0.012517607706540623\n",
            "train loss:0.08223339297977578\n",
            "train loss:0.048275055722130956\n",
            "train loss:0.02106662447346634\n",
            "train loss:0.02223923744531331\n",
            "train loss:0.05273163770043065\n",
            "train loss:0.018122531984937122\n",
            "train loss:0.027044954131275948\n",
            "train loss:0.04528240045572891\n",
            "train loss:0.04989596399599159\n",
            "train loss:0.01699244396540533\n",
            "train loss:0.02249997494757786\n",
            "train loss:0.09493256226720571\n",
            "train loss:0.010151285899376649\n",
            "train loss:0.035668743756252896\n",
            "train loss:0.027670974029609708\n",
            "train loss:0.02121826819425222\n",
            "train loss:0.02290566117972158\n",
            "train loss:0.054887870309335944\n",
            "train loss:0.04061921684094334\n",
            "=== epoch:17, train acc:0.986, test acc:0.956 ===\n",
            "train loss:0.018803882026162264\n",
            "train loss:0.051936431328845595\n",
            "train loss:0.03307184997493222\n",
            "train loss:0.028238264641919572\n",
            "train loss:0.05972821041401474\n",
            "train loss:0.019993935359788603\n",
            "train loss:0.016987865039183522\n",
            "train loss:0.038764014982868286\n",
            "train loss:0.011302619866516246\n",
            "train loss:0.030254961673039336\n",
            "train loss:0.00950970560437198\n",
            "train loss:0.03554447342303152\n",
            "train loss:0.034056485125613906\n",
            "train loss:0.04613497748205013\n",
            "train loss:0.011025173005132154\n",
            "train loss:0.030965104392025294\n",
            "train loss:0.01919232104899628\n",
            "train loss:0.03155769500578939\n",
            "train loss:0.014505518837287226\n",
            "train loss:0.026426349926219696\n",
            "train loss:0.027982618675720855\n",
            "train loss:0.06909203977687098\n",
            "train loss:0.028735713079986586\n",
            "train loss:0.027374638066470353\n",
            "train loss:0.036620308779420914\n",
            "train loss:0.02178902302967354\n",
            "train loss:0.01717217871630977\n",
            "train loss:0.05976173677858377\n",
            "train loss:0.033284794314429954\n",
            "train loss:0.013853397397686859\n",
            "train loss:0.09961131368002184\n",
            "train loss:0.018200494828547326\n",
            "train loss:0.011955895622716664\n",
            "train loss:0.03475244489406936\n",
            "train loss:0.019820111734741793\n",
            "train loss:0.0651418175222775\n",
            "train loss:0.042571335020579644\n",
            "train loss:0.02633766251769272\n",
            "train loss:0.040978847377558164\n",
            "train loss:0.017301366479225616\n",
            "train loss:0.012742835960348009\n",
            "train loss:0.03898940287047078\n",
            "train loss:0.04262988590875989\n",
            "train loss:0.027472722891316345\n",
            "train loss:0.008558804147958987\n",
            "train loss:0.02691116229168423\n",
            "train loss:0.03955759753981596\n",
            "train loss:0.018501530506952857\n",
            "train loss:0.034488519185736484\n",
            "train loss:0.02851151411887492\n",
            "=== epoch:18, train acc:0.991, test acc:0.962 ===\n",
            "train loss:0.02279778882088887\n",
            "train loss:0.023018234468875236\n",
            "train loss:0.027415967903056476\n",
            "train loss:0.019590524889377084\n",
            "train loss:0.06295281685916848\n",
            "train loss:0.017681956303821415\n",
            "train loss:0.025202412968824518\n",
            "train loss:0.05998915415802328\n",
            "train loss:0.01909168736894268\n",
            "train loss:0.05545069655825841\n",
            "train loss:0.0164441969128482\n",
            "train loss:0.026814851684920912\n",
            "train loss:0.02015869403840258\n",
            "train loss:0.04406133646070983\n",
            "train loss:0.020631837145119864\n",
            "train loss:0.014731150010723974\n",
            "train loss:0.014330018914401716\n",
            "train loss:0.028901993938769757\n",
            "train loss:0.057026360650824115\n",
            "train loss:0.008509590529311243\n",
            "train loss:0.03379444778966403\n",
            "train loss:0.029145329252450117\n",
            "train loss:0.03735428049800865\n",
            "train loss:0.01317045546408858\n",
            "train loss:0.039599504257683764\n",
            "train loss:0.014163898549835774\n",
            "train loss:0.031382461546964606\n",
            "train loss:0.050131672038938674\n",
            "train loss:0.008256633169099565\n",
            "train loss:0.019592764120561938\n",
            "train loss:0.028897248209428218\n",
            "train loss:0.022241380944405323\n",
            "train loss:0.03548517013072782\n",
            "train loss:0.014991387177409365\n",
            "train loss:0.0162996589068214\n",
            "train loss:0.02100482773612747\n",
            "train loss:0.015047635375346213\n",
            "train loss:0.020268361744710715\n",
            "train loss:0.006944978744279589\n",
            "train loss:0.012176874536942708\n",
            "train loss:0.011999818115706789\n",
            "train loss:0.015525003294819961\n",
            "train loss:0.022687647410715116\n",
            "train loss:0.010465424090379184\n",
            "train loss:0.049707717637577614\n",
            "train loss:0.013452724408734953\n",
            "train loss:0.017606423401568094\n",
            "train loss:0.008593451160243652\n",
            "train loss:0.019081522531656164\n",
            "train loss:0.018358446332556008\n",
            "=== epoch:19, train acc:0.993, test acc:0.959 ===\n",
            "train loss:0.024265940410732073\n",
            "train loss:0.030137211656244375\n",
            "train loss:0.034599122819488806\n",
            "train loss:0.018348361568338623\n",
            "train loss:0.013503550573943259\n",
            "train loss:0.025663286517541458\n",
            "train loss:0.013121322630001\n",
            "train loss:0.015826890384368904\n",
            "train loss:0.03155732660155163\n",
            "train loss:0.015027505239364455\n",
            "train loss:0.018547626777655996\n",
            "train loss:0.010313310980140109\n",
            "train loss:0.045661847608749244\n",
            "train loss:0.011834437232589402\n",
            "train loss:0.06532277538062838\n",
            "train loss:0.018895276600967483\n",
            "train loss:0.013010334266006118\n",
            "train loss:0.01935110794686343\n",
            "train loss:0.012268049399659697\n",
            "train loss:0.02850597199331447\n",
            "train loss:0.007443140797557649\n",
            "train loss:0.05730412579441279\n",
            "train loss:0.05142227019715385\n",
            "train loss:0.02660292249174868\n",
            "train loss:0.030076963119683674\n",
            "train loss:0.014733357955288531\n",
            "train loss:0.026687291382154785\n",
            "train loss:0.037783643403683564\n",
            "train loss:0.011067308366528267\n",
            "train loss:0.009792656948055865\n",
            "train loss:0.11407147439016624\n",
            "train loss:0.05201404284607312\n",
            "train loss:0.02292735001941034\n",
            "train loss:0.01778979283022707\n",
            "train loss:0.010809366357575099\n",
            "train loss:0.011484882454144361\n",
            "train loss:0.01154426286194451\n",
            "train loss:0.04174033277849757\n",
            "train loss:0.009953389786241969\n",
            "train loss:0.026588247307841057\n",
            "train loss:0.04644733122533672\n",
            "train loss:0.031164197774509347\n",
            "train loss:0.018318530171418292\n",
            "train loss:0.026792268426340304\n",
            "train loss:0.06303661438812315\n",
            "train loss:0.013139754207235121\n",
            "train loss:0.016774041557781526\n",
            "train loss:0.0155765379539323\n",
            "train loss:0.018351930759693876\n",
            "train loss:0.008296327297630312\n",
            "=== epoch:20, train acc:0.993, test acc:0.96 ===\n",
            "train loss:0.03022173417671154\n",
            "train loss:0.04133477071435504\n",
            "train loss:0.024992974081070818\n",
            "train loss:0.017240759295133114\n",
            "train loss:0.014459449515426919\n",
            "train loss:0.03577221846203552\n",
            "train loss:0.033047066294069304\n",
            "train loss:0.015791471700035264\n",
            "train loss:0.06367445441653972\n",
            "train loss:0.02641452513323204\n",
            "train loss:0.010837286890886704\n",
            "train loss:0.014441987046936344\n",
            "train loss:0.036729598238685074\n",
            "train loss:0.010191092912609166\n",
            "train loss:0.027011028881872817\n",
            "train loss:0.01340357318411968\n",
            "train loss:0.02944714984966602\n",
            "train loss:0.023655362892267023\n",
            "train loss:0.04147138853413984\n",
            "train loss:0.010603300707474075\n",
            "train loss:0.011786606189723272\n",
            "train loss:0.024948558263230503\n",
            "train loss:0.017871792958659302\n",
            "train loss:0.023435365851523806\n",
            "train loss:0.03185084980641749\n",
            "train loss:0.005385529943625501\n",
            "train loss:0.010543337854249912\n",
            "train loss:0.03375965690440133\n",
            "train loss:0.03228988790749969\n",
            "train loss:0.030032010188188987\n",
            "train loss:0.0170434433492326\n",
            "train loss:0.021252200667251447\n",
            "train loss:0.015805774475853043\n",
            "train loss:0.015155387486238863\n",
            "train loss:0.009436441814887031\n",
            "train loss:0.06557629194273919\n",
            "train loss:0.042098974289413754\n",
            "train loss:0.009386779717094455\n",
            "train loss:0.011490122870664796\n",
            "train loss:0.03024277462640136\n",
            "train loss:0.0735834376766524\n",
            "train loss:0.026348628484665154\n",
            "train loss:0.021690212803401464\n",
            "train loss:0.03249686726661258\n",
            "train loss:0.010453704215506059\n",
            "train loss:0.05308162579837734\n",
            "train loss:0.022646515690377674\n",
            "train loss:0.038293748236737826\n",
            "train loss:0.021291492760554212\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.96\n",
            "Saved Network Parameters!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}